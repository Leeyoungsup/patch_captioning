{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Time: 2025-01-08 10:56:48\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import timm\n",
    "from glob import glob\n",
    "from rouge_score import rouge_scorer\n",
    "import random\n",
    "from torchvision.transforms import ToTensor\n",
    "import time\n",
    "import json\n",
    "import warnings\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from rouge_score import rouge_scorer\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "nltk.download('punkt')\n",
    "tf = ToTensor()\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "# Device configurationresul\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "start_time = time.time()\n",
    "print(\"Start Time:\", time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(start_time)))\n",
    "params={'image_size':1024,\n",
    "        'lr':2e-4,\n",
    "        'beta1':0.5,\n",
    "        'beta2':0.999,\n",
    "        'batch_size':8,\n",
    "        'epochs':50,\n",
    "        'data_path':'../../data/원시/010.위암 병리 이미지 및 판독문 합성 데이터/',\n",
    "        'train_json':'../../data/원시/010.위암 병리 이미지 및 판독문 합성 데이터/train/2.라벨링/',\n",
    "        'test_json':'../../data/원시/010.위암 병리 이미지 및 판독문 합성 데이터/test/2.라벨링/',\n",
    "        'vocab_path':'../../data/원시/010.위암 병리 이미지 및 판독문 합성 데이터/vocab.pkl',\n",
    "        'embed_size':300,\n",
    "        'hidden_size':256,\n",
    "        'num_layers':4,}\n",
    "model_name='efficientnetv2_s'\n",
    "aa='ST'\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"COCO Custom Dataset compatible with torch.utils.data.DataLoader.\"\"\"\n",
    "    def __init__(self,data_list, data_path,image_size, caption_list, class_dataset, vocab, transform=None):\n",
    "        \"\"\"Set the path for images, captions and vocabulary wrapper.\n",
    "        \n",
    "        Args:\n",
    "            root: image directory.\n",
    "            json: coco annotation file path.\n",
    "            vocab: vocabulary wrapper.\n",
    "            transform: image transformer.\n",
    "        \"\"\"\n",
    "        self.data_path=data_path\n",
    "        self.caption_list= caption_list\n",
    "        self.class_dataset=class_dataset\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "        self.image_size=image_size\n",
    "        self.data_list=data_list\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Returns one data pair (image and caption).\"\"\"\n",
    "        caption = self.caption_list[index]\n",
    "        vocab = self.vocab\n",
    "        images = self.data_list[index]\n",
    "        # Convert caption (string) to word ids.\n",
    "        path=self.data_path[index]\n",
    "        tokens = nltk.tokenize.word_tokenize(str(caption).lower())\n",
    "        caption = []\n",
    "        caption.append(vocab('<start>'))\n",
    "        caption.extend([vocab(token) for token in tokens])\n",
    "        caption.append(vocab('<end>'))\n",
    "        target = torch.Tensor(caption)\n",
    "        return images, target,path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "    \n",
    "\n",
    "class Vocabulary(object):\n",
    "    \"\"\"Simple vocabulary wrapper.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            return self.word2idx['<unk>']\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "def collate_fn(data):\n",
    "    \"\"\"Creates mini-batch tensors from the list of tuples (image, caption).\n",
    "    \n",
    "    We should build custom collate_fn rather than using default collate_fn, \n",
    "    because merging caption (including padding) is not supported in default.\n",
    "\n",
    "    Args:\n",
    "        data: list of tuple (image, caption). \n",
    "            - image: torch tensor of shape (3, 256, 256).\n",
    "            - caption: torch tensor of shape (?); variable length.\n",
    "\n",
    "    Returns:\n",
    "        images: torch tensor of shape (batch_size, 3, 256, 256).\n",
    "        targets: torch tensor of shape (batch_size, padded_length).\n",
    "        lengths: list; valid length for each padded caption.\n",
    "    \"\"\"\n",
    "    # Sort a data list by caption length (descending order).\n",
    "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    images, captions,path = zip(*data)\n",
    "\n",
    "    # Merge images (from tuple of 3D tensor to 4D tensor).\n",
    "    images = torch.stack(images, 0)\n",
    "\n",
    "    # Merge captions (from tuple of 1D tensor to 2D tensor).\n",
    "    lengths = [len(cap) for cap in captions]\n",
    "    targets = torch.zeros(len(captions), max(lengths)).long()\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        targets[i, :end] = cap[:end]        \n",
    "    return images, targets,path, lengths\n",
    "\n",
    "def idx2word(vocab, indices):\n",
    "    sentence = []\n",
    "    \n",
    "    aa=indices.cpu().numpy()\n",
    "    \n",
    "    for index in aa:\n",
    "        word = vocab.idx2word[index]\n",
    "        sentence.append(word)\n",
    "    return sentence\n",
    "def word2sentence(words_list):\n",
    "    sentence=''\n",
    "    for word in words_list:\n",
    "        if word.isalnum():\n",
    "            sentence+=' '+word\n",
    "        else:\n",
    "            sentence+=word\n",
    "    return sentence\n",
    "\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    \"\"\"Feature extoractor block\"\"\"\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        cnn1= timm.create_model(model_name)\n",
    "        self.feature_ex = nn.Sequential(*list(cnn1.children())[:-1])\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        features = self.feature_ex(inputs)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "class AttentionMILModel(nn.Module):\n",
    "    def __init__(self, num_classes, image_feature_dim,feature_extractor_scale1: FeatureExtractor):\n",
    "        super(AttentionMILModel, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.image_feature_dim = image_feature_dim\n",
    "\n",
    "        # Remove the classification head of the CNN model\n",
    "        self.feature_extractor = feature_extractor_scale1\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(image_feature_dim, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "        # Classification layer\n",
    "        self.classification_layer = nn.Linear(image_feature_dim, num_classes)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        batch_size, channels, height, width = inputs.size()\n",
    "        \n",
    "        # Flatten the inputs\n",
    "        inputs = inputs.view(-1, channels, height, width)\n",
    "        \n",
    "        # Feature extraction using the pre-trained CNN\n",
    "        features = self.feature_extractor(inputs)  # Shape: (batch_size , 2048, 1, 1)\n",
    "        \n",
    "        # Reshape features\n",
    "        features = features.view(batch_size, -1)  # Shape: (batch_size, num_tiles, 2048)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Classification layer\n",
    "        logits = self.classification_layer(features)  # Shape: (batch_size, num_classes)\n",
    "        \n",
    "        return logits  \n",
    "\n",
    "class DecoderTransformer(nn.Module):\n",
    "    def __init__(self, embed_size, vocab_size, num_heads, hidden_size, num_layers, max_seq_length=100):\n",
    "        super(DecoderTransformer, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, max_seq_length, embed_size))\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # Transformer Decoder\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_size, nhead=num_heads, dim_feedforward=hidden_size)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        self.linear = nn.Linear(embed_size, vocab_size)\n",
    "        \n",
    "    def forward(self, features, captions, teacher_forcing_ratio=1.0):\n",
    "        \"\"\"\n",
    "        features: (batch_size, embed_size)\n",
    "        captions: (batch_size, max_seq_length)\n",
    "        \"\"\"\n",
    "        batch_size = features.size(0)\n",
    "        max_seq_length = captions.size(1)\n",
    "        \n",
    "        # Output 저장을 위한 텐서 초기화\n",
    "        outputs = torch.zeros(batch_size, max_seq_length, self.vocab_size).to(features.device)\n",
    "        \n",
    "        # features를 memory로 사용\n",
    "        memory = features.unsqueeze(0)  # (1, batch_size, embed_size)\n",
    "        \n",
    "        # 첫 번째 입력 토큰은 <start> 토큰\n",
    "        input_caption = captions[:, 0].unsqueeze(1)  # (batch_size, 1)\n",
    "        \n",
    "        for t in range(1, max_seq_length):\n",
    "            # 임베딩 및 포지셔널 인코딩 적용\n",
    "            input_embedded = self.embed(input_caption) + self.positional_encoding[:, :input_caption.size(1), :]\n",
    "            input_embedded = input_embedded.permute(1, 0, 2)  # (seq_len, batch_size, embed_size)\n",
    "            \n",
    "            # 타겟 마스크 생성\n",
    "            tgt_mask = self.generate_square_subsequent_mask(input_embedded.size(0)).to(features.device)\n",
    "            \n",
    "            # Transformer 디코더에 입력\n",
    "            transformer_output = self.transformer_decoder(input_embedded, memory, tgt_mask=tgt_mask)\n",
    "            transformer_output = transformer_output.permute(1, 0, 2)\n",
    "            \n",
    "            # 현재 시간 스텝의 출력 계산\n",
    "            output = self.linear(transformer_output[:, -1, :])  # (batch_size, vocab_size)\n",
    "            outputs[:, t, :] = output  # 출력 저장\n",
    "            \n",
    "            # 다음 입력 결정 (교사 강요 비율에 따라)\n",
    "            use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "            if use_teacher_forcing:\n",
    "                # 실제 캡션의 다음 토큰 사용\n",
    "                next_input = captions[:, t].unsqueeze(1)\n",
    "            else:\n",
    "                # 모델의 예측 사용\n",
    "                _, predicted = output.max(1)\n",
    "                next_input = predicted.unsqueeze(1)\n",
    "            \n",
    "            # 다음 입력을 input_caption에 추가\n",
    "            input_caption = torch.cat([input_caption, next_input], dim=1)\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        \"\"\"시퀀스의 순차적인 마스크 생성\"\"\"\n",
    "        mask = torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "        return mask\n",
    "\n",
    "    def sample(self, features, max_seq_length=None):\n",
    "        \"\"\"Greedy Search 방식으로 시퀀스를 샘플링합니다.\"\"\"\n",
    "        if max_seq_length is None:\n",
    "            max_seq_length = self.max_seq_length\n",
    "        \n",
    "        batch_size = features.size(0)\n",
    "        sampled_ids = []\n",
    "        \n",
    "        # 첫 번째 토큰은 <start> 토큰\n",
    "        input_caption = torch.ones(batch_size, 1).long().to(features.device)\n",
    "        memory = features.unsqueeze(0)  # (1, batch_size, embed_size)\n",
    "        \n",
    "        for _ in range(max_seq_length):\n",
    "            input_embedded = self.embed(input_caption) + self.positional_encoding[:, :input_caption.size(1), :]\n",
    "            input_embedded = input_embedded.permute(1, 0, 2)\n",
    "            tgt_mask = self.generate_square_subsequent_mask(input_embedded.size(0)).to(features.device)\n",
    "            transformer_output = self.transformer_decoder(input_embedded, memory, tgt_mask=tgt_mask)\n",
    "            transformer_output = transformer_output.permute(1, 0, 2)\n",
    "            output = self.linear(transformer_output[:, -1, :])  # (batch_size, vocab_size)\n",
    "            _, predicted = output.max(1)\n",
    "            sampled_ids.append(predicted)\n",
    "            input_caption = torch.cat([input_caption, predicted.unsqueeze(1)], dim=1)\n",
    "        \n",
    "        sampled_ids = torch.stack(sampled_ids, 1)\n",
    "        return sampled_ids\n",
    "def bleu_n(pred_words_list, label_words_list):\n",
    "    # Define a smoothing function to avoid warnings\n",
    "    smoothing = SmoothingFunction().method1\n",
    "    \n",
    "    # BLEU@1 calculation\n",
    "    bleu1 = sentence_bleu([label_words_list], pred_words_list, weights=(1, 0, 0, 0), smoothing_function=smoothing)\n",
    "    \n",
    "    # BLEU@2 calculation\n",
    "    bleu2 = sentence_bleu([label_words_list], pred_words_list, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothing)\n",
    "    \n",
    "    # BLEU@3 calculation\n",
    "    bleu3 = sentence_bleu([label_words_list], pred_words_list, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smoothing)\n",
    "    \n",
    "    # BLEU@4 calculation\n",
    "    bleu4 = sentence_bleu([label_words_list], pred_words_list, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing)\n",
    "    \n",
    "    return bleu1, bleu2, bleu3, bleu4\n",
    "\n",
    "def rouge_scores(pred_sentence, label_sentence):\n",
    "    # Initialize the ROUGE scorer\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    \n",
    "    # Calculate the scores\n",
    "    scores = scorer.score(label_sentence, pred_sentence)\n",
    "    \n",
    "    # Extract the precision, recall, and f1 scores for each ROUGE metric\n",
    "    rouge1 = scores['rouge1']\n",
    "    rouge2 = scores['rouge2']\n",
    "    rougeL = scores['rougeL']\n",
    "    \n",
    "    # Return the ROUGE scores\n",
    "    return rouge1, rouge2, rougeL\n",
    "\n",
    "def calculate_cider(references, hypotheses):\n",
    "    \"\"\"\n",
    "    CIDEr 점수를 계산합니다.\n",
    "\n",
    "    Args:\n",
    "        references (list of str): 각 이미지의 참조 설명 문장들.\n",
    "        hypotheses (list of str): 각 이미지에 대한 생성된 설명 문장들.\n",
    "\n",
    "    Returns:\n",
    "        float: CIDEr 점수의 평균값.\n",
    "        list of float: 각 이미지에 대한 CIDEr 점수.\n",
    "    \"\"\"\n",
    "    # pycocoevalcap 라이브러리는 references와 hypotheses의 형식을 다음과 같이 요구합니다.\n",
    "    gts = {i: [ref] for i, ref in enumerate(references)}\n",
    "    res = {i: [hypothesis] for i, hypothesis in enumerate(hypotheses)}\n",
    "\n",
    "    # CIDEr 계산\n",
    "    cider_scorer = Cider()\n",
    "    cider_score, cider_per_image_scores = cider_scorer.compute_score(gts, res)\n",
    "\n",
    "    return cider_score, cider_per_image_scores\n",
    "\n",
    "# 1/2-gram Precision 계산 함수\n",
    "def calculate_ngram_precision(references, hypotheses, n):\n",
    "    \"\"\"\n",
    "    Calculates n-gram precision for a set of generated captions against reference captions.\n",
    "\n",
    "    Args:\n",
    "        references (list of str): List of reference captions.\n",
    "        hypotheses (list of str): List of generated captions.\n",
    "        n (int): The n-gram size.\n",
    "\n",
    "    Returns:\n",
    "        list of float: Precision scores for each hypothesis.\n",
    "    \"\"\"\n",
    "    precision_scores = []\n",
    "\n",
    "    for hypothesis, reference in zip(hypotheses, references):\n",
    "        # Tokenize hypothesis and reference\n",
    "        hypothesis_tokens = hypothesis.split()\n",
    "        reference_tokens = reference.split()\n",
    "\n",
    "        # Generate n-grams\n",
    "        hypothesis_ngrams = list(ngrams(hypothesis_tokens, n))\n",
    "        reference_ngrams = list(ngrams(reference_tokens, n))\n",
    "\n",
    "        # Count n-gram occurrences\n",
    "        hypothesis_counts = Counter(hypothesis_ngrams)\n",
    "        reference_counts = Counter(reference_ngrams)\n",
    "\n",
    "        # Calculate overlapping n-grams\n",
    "        overlap = sum((hypothesis_counts & reference_counts).values())\n",
    "        total = sum(hypothesis_counts.values())\n",
    "\n",
    "        # Precision calculation\n",
    "        precision = overlap / total if total > 0 else 0.0\n",
    "        precision_scores.append(precision)\n",
    "\n",
    "    return precision_scores\n",
    "\n",
    "# Mean and SD Calculation for Average 1/2-gram Precision\n",
    "def calculate_mean_sd_average_precision(references, hypotheses):\n",
    "    \"\"\"\n",
    "    Calculates mean and standard deviation for the average of 1-gram and 2-gram precision.\n",
    "\n",
    "    Args:\n",
    "        references (list of str): List of reference captions.\n",
    "        hypotheses (list of str): List of generated captions.\n",
    "\n",
    "    Returns:\n",
    "        dict: Mean and SD for the average 1/2-gram precision.\n",
    "    \"\"\"\n",
    "    avg_precisions = []\n",
    "\n",
    "    for hypothesis, reference in zip(hypotheses, references):\n",
    "        precisions = []\n",
    "        for n in [1, 2]:\n",
    "            ngram_precision = calculate_ngram_precision([reference], [hypothesis], n)\n",
    "            precisions.append(ngram_precision[0])\n",
    "        avg_precisions.append(np.mean(precisions))\n",
    "\n",
    "    mean_avg_precision = np.mean(avg_precisions)\n",
    "    sd_avg_precision = np.std(avg_precisions)\n",
    "\n",
    "    return {\"mean\": mean_avg_precision, \"sd\": sd_avg_precision}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 19/300 [00:02<00:30,  9.28it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m test_list\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(test_json_list),\u001b[38;5;241m3\u001b[39m,params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_size\u001b[39m\u001b[38;5;124m'\u001b[39m],params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_size\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(test_image_list))):\n\u001b[0;32m---> 14\u001b[0m     image\u001b[38;5;241m=\u001b[39mtransform(\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_image_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     15\u001b[0m     test_list[i]\u001b[38;5;241m=\u001b[39mimage\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(test_json_list[i], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8-sig\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.10/site-packages/PIL/Image.py:2293\u001b[0m, in \u001b[0;36mImage.resize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2290\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreducing_gap must be 1.0 or greater\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2291\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m-> 2293\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m box \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2295\u001b[0m     box \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.10/site-packages/PIL/ImageFile.py:293\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[1;32m    292\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 293\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open(params['vocab_path'], 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "transform = transforms.Compose([ \n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                             (0.229, 0.224, 0.225))])\n",
    "\n",
    "test_json_list=glob(params['test_json']+'**/*.json')\n",
    "test_image_list=[f.replace('2.라벨링', '1.원시데이터') for f in test_json_list]\n",
    "test_image_list=[f.replace('.json', '.png') for f in test_image_list]\n",
    "test_caption_list=[]\n",
    "test_list=torch.zeros(len(test_json_list),3,params['image_size'],params['image_size'])\n",
    "for i in tqdm(range(len(test_image_list))):\n",
    "    image=transform(Image.open(test_image_list[i]).resize((params['image_size'],params['image_size'])))\n",
    "    test_list[i]=image\n",
    "    with open(test_json_list[i], 'r', encoding='utf-8-sig') as file:\n",
    "        data = json.load(file)\n",
    "    test_caption_list.append(str(data['content']['file']['patch_discription']))\n",
    "\n",
    "test_dataset=CustomDataset(test_list,test_image_list,params['image_size'],test_caption_list,'val',vocab,transform=transform)\n",
    "test_dataloader=DataLoader(test_dataset,batch_size=params['batch_size'],shuffle=False,collate_fn=collate_fn,drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../model/ST_encoder_resnet50.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m model_param \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(decoder\u001b[38;5;241m.\u001b[39mparameters()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(encoder\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[1;32m      7\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model_param, lr\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m], betas\u001b[38;5;241m=\u001b[39m(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeta1\u001b[39m\u001b[38;5;124m'\u001b[39m], params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeta2\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m----> 8\u001b[0m encoder\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../../model/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43maa\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_encoder_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      9\u001b[0m decoder\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../model/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maa\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_decoder_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m,map_location\u001b[38;5;241m=\u001b[39mdevice))\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.10/site-packages/torch/serialization.py:997\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    995\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 997\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    998\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    999\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.10/site-packages/torch/serialization.py:444\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 444\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.10/site-packages/torch/serialization.py:425\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 425\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../model/ST_encoder_resnet50.pth'"
     ]
    }
   ],
   "source": [
    "Feature_Extractor=FeatureExtractor()\n",
    "encoder = AttentionMILModel(params['embed_size'], 1280, Feature_Extractor).to(device)\n",
    "decoder = DecoderTransformer(params['embed_size'], len(vocab), 15, params['hidden_size'], params['num_layers']).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model_param = list(decoder.parameters()) + list(encoder.parameters())\n",
    "optimizer = torch.optim.Adam(model_param, lr=params['lr'], betas=(params['beta1'], params['beta2']))\n",
    "encoder.load_state_dict(torch.load(f'../../model/{aa}_encoder_{model_name}.pth',map_location=device))\n",
    "decoder.load_state_dict(torch.load(f'../../model/{aa}_decoder_{model_name}.pth',map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_labels = []\n",
    "total_predictions = []\n",
    "total_bleu=[]\n",
    "total_meteor=[]\n",
    "\n",
    "total_Rogue=[]\n",
    "total_reference=[]\n",
    "total_candidate=[]\n",
    "test_df=pd.DataFrame(columns=['Path','Predicted','label','BLEU1','BLEU2','BLEU3','BLEU4','Rouge1','Rouge2','RougeL'])\n",
    "with torch.no_grad():\n",
    "    test_count = 0\n",
    "    test_loss = 0.0 \n",
    "    test_bleu_score = 0.0\n",
    "    test_tq = tqdm(test_dataloader)\n",
    "    for images, captions,path, lengths in test_tq:\n",
    "        \n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "\n",
    "        # Encoder를 통해 특징 추출\n",
    "        features = encoder(images)\n",
    "        # 캡션 생성 (교사 강요 없이)\n",
    "        sampled_ids = decoder.sample(features)\n",
    "        \n",
    "        # BLEU 점수 계산\n",
    "        for i in range(images.size(0)):\n",
    "            test_count += 1\n",
    "            predicted_caption = idx2word(vocab, sampled_ids[i])\n",
    "            target_caption = idx2word(vocab, captions[i])\n",
    "            \n",
    "            # 특수 토큰 제거\n",
    "            predicted_caption = [word for word in predicted_caption if word not in ['<start>', '<end>', '<pad>','<unk>']]\n",
    "            target_caption = [word for word in target_caption if word not in ['<start>', '<end>', '<pad>','<unk>']]\n",
    "            predicted_sentence=word2sentence(predicted_caption)\n",
    "            target_sentence=word2sentence(target_caption)\n",
    "            reference = word2sentence(target_caption )\n",
    "            candidate = word2sentence(predicted_caption)\n",
    "            total_reference.append(reference)\n",
    "            total_candidate.append(candidate)\n",
    "            bleu1,bleu2,bleu3,bleu4=bleu_n(predicted_caption,target_caption)\n",
    "            rouge1, rouge2, rougeL = rouge_scores(candidate, reference)\n",
    "            total_bleu.append([bleu1,bleu2,bleu3,bleu4])\n",
    "            total_Rogue.append([rouge1, rouge2, rougeL])\n",
    "            meteor_score_value = meteor_score([target_caption], predicted_caption)\n",
    "            total_meteor.append(meteor_score_value)\n",
    "            # BLEU-4 점수 계산\n",
    "            bleu_score = sentence_bleu([target_caption], predicted_caption, weights=(1, 0, 0, 0))\n",
    "            test_bleu_score += bleu_score\n",
    "            bleu_11=bleu_n(predicted_caption,target_caption)\n",
    "            # ROUGE 점수 계산\n",
    "            rouge_score = rouge_scores(predicted_sentence, target_sentence)\n",
    "            # 예측과 정답 문장 저장\n",
    "            total_labels.append(target_sentence)\n",
    "            total_predictions.append(predicted_sentence)\n",
    "\n",
    "            test_df.loc[len(test_df)]={'Path':os.path.basename(path[i]),'Predicted':predicted_sentence,'label':target_sentence,'BLEU1':bleu_11[0],'BLEU2':bleu_11[1],'BLEU3':bleu_11[2],'BLEU4':bleu_11[3],'Rouge1':rouge_score[0],'Rouge2':rouge_score[1],'RougeL':rouge_score[2]}\n",
    "        test_tq.set_description(f\"test BLEU-1: {test_bleu_score/(test_count):.4f}\")\n",
    "    average_cider_score, cider_scores = calculate_cider(total_reference, total_candidate)\n",
    "    average_result = calculate_mean_sd_average_precision(total_reference, total_candidate)\n",
    "    Avg_gram_mean=average_result['mean']\n",
    "    Avg_gram_sd=average_result['sd']\n",
    "test_df.to_csv(f'../../result/{aa}_result_{model_name}.csv',index=False)\n",
    "\n",
    "print(f'Bleu-1:{np.array(total_bleu)[:,0].mean():.4f}+-{np.array(total_bleu)[:,0].std():.4f} \\nBleu-2:{np.array(total_bleu)[:,1].mean():.4f}+-{np.array(total_bleu)[:,1].std():.4f} \\nBleu-3:{np.array(total_bleu)[:,2].mean():.4f}+-{np.array(total_bleu)[:,2].std():.4f} \\nBleu-4:{np.array(total_bleu)[:,3].mean():.4f}+-{np.array(total_bleu)[:,3].std():.4f} \\nRogue-1:{np.array(total_Rogue)[:,0].mean():.4f}+-{np.array(total_Rogue)[:,0].std():.4f} \\nRogue-2:{np.array(total_Rogue)[:,1].mean():.4f}+-{np.array(total_Rogue)[:,1].std():.4f} \\nRogue-L:{np.array(total_Rogue)[:,2].mean():.4f}+-{np.array(total_Rogue)[:,2].std():.4f}')\n",
    "end_time = time.time()\n",
    "print(\"\\nEnd Time:\", time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(end_time)))\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Elapsed Time: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'BLEU@4 : {np.array(total_bleu)[:,3].mean():.4f}+-{np.array(total_bleu)[:,3].std():.4f} \\nRouge-L : {np.array(total_Rogue)[:,2].mean():.4f}+-{np.array(total_Rogue)[:,2].std():.4f} \\nMETEOR : {np.array(total_meteor).mean():.4f}+-{np.array(total_meteor).std():.4f} \\nCIDEr : {average_cider_score:.4f}+-{np.array(cider_scores).std():.4f} \\nAvg-gram : {Avg_gram_mean:.4f}+-{Avg_gram_sd:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeeYS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
